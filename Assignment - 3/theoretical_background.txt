================================================================================
NON-STATIONARY 4-ARM BANDIT: EXPERIMENT RESULTS & ANALYSIS
================================================================================

WHAT I DID:
-----------
I compared two learning methods in a non-stationary 4-arm bandit problem:
1. Sample Average Method (α = 1/n) - decreasing step-size
2. Constant Step-Size Method (α = 0.1) - fixed step-size

Setup: ε = 0.1, 10,000 steps, 500 independent runs averaged

Environment: q*(a) values start at 0 and drift via random walk N(0, 0.01) each step


================================================================================
RESULTS FROM MY EXPERIMENT
================================================================================

PLOT 1: AVERAGE REWARD OVER TIME
---------------------------------
What I Observe:
- Constant α (blue line): Maintains higher, stable average reward throughout
- Sample Average (red line): Lower reward that gradually declines over time
- Clear performance gap that widens as steps increase

PLOT 2: % OPTIMAL ACTION OVER TIME  
-----------------------------------
What I Observe:
- Constant α (blue line): Consistently selects optimal action ~70-80% of time
- Sample Average (red line): Poor performance ~30-40%, struggles to track optimal action
- Constant α adapts quickly when optimal action changes due to drift


NUMERICAL RESULTS:
------------------
Method              | Avg Reward | % Optimal Action
--------------------|------------|-----------------
Sample Average      |   ~0.1-0.3 |      30-40%
Constant α=0.1      |   ~0.6-0.8 |      70-80%

Constant step-size achieves 2-3x better performance!


================================================================================
WHY THIS MATCHES THEORY
================================================================================

THE PROBLEM WITH SAMPLE AVERAGING IN NON-STATIONARY ENVIRONMENTS:
------------------------------------------------------------------
As I take more actions, α = 1/n becomes very small:
- After 1000 steps: α = 0.001 (only 0.1% update from new reward)
- After 5000 steps: α = 0.0002 (almost no learning!)
- My Q estimates get "stuck" on old, outdated information
- Cannot adapt when q*(a) changes

This is WHY the red line in both plots performs poorly and degrades over time.


THE ADVANTAGE OF CONSTANT α = 0.1:
-----------------------------------
Every reward updates Q by 10% of the error, always:
- Continuous learning throughout all 10,000 steps
- "Forgets" old information through exponential decay
- Effective memory window ≈ 10 steps
- Can track q*(a) drift effectively

This is WHY the blue line maintains strong performance throughout.


MATHEMATICAL INSIGHT:
---------------------
Sample Average: Q(a) = (1/n) × [R₁ + R₂ + ... + Rₙ]
- All rewards weighted equally (1/n each)
- Old rewards from 5000 steps ago still influence current Q
- In non-stationary problem, old rewards are misleading!

Constant α: Q(a) = weighted sum with exponentially decaying weights
- Recent reward: weight = 0.1
- Reward 10 steps ago: weight ≈ 0.035 (decayed by 65%)
- Reward 50 steps ago: weight ≈ 0.0005 (almost forgotten)
- Recent information matters most!


================================================================================
KEY CONCEPTS FOR VIVA
================================================================================

1. MULTI-ARMED BANDIT PROBLEM
------------------------------
- Simplest RL problem: single state, multiple actions
- Each action a has true value q*(a)
- Goal: Learn which action gives highest expected reward
- Rewards sampled from N(q*(a), 1) - noisy, need multiple samples


2. STATIONARY vs NON-STATIONARY
--------------------------------
STATIONARY: q*(a) values don't change
→ Use sample average (all data equally valuable)

NON-STATIONARY: q*(a) values drift over time (my experiment)
→ Use constant step-size (recent data more valuable)


3. EPSILON-GREEDY STRATEGY
---------------------------
Balances exploration vs exploitation:
- 10% of time: Explore (random action) to discover better options
- 90% of time: Exploit (greedy action) to maximize reward
- Without exploration, would get stuck on initially good actions


4. THE UPDATE RULE
------------------
Q(a) ← Q(a) + α[R - Q(a)]

NewEstimate = OldEstimate + StepSize × PredictionError

- α controls learning rate
- [R - Q(a)] is how wrong I was
- Incrementally moves Q toward observed rewards


5. WHY CONSTANT α WINS IN MY EXPERIMENT
----------------------------------------
Non-stationary environment needs:
✓ Continuous adaptation (constant α provides this)
✓ Recent data prioritization (exponential weighting)
✓ Ability to "forget" outdated information

Sample average fails because:
✗ Learning rate → 0 (cannot adapt)
✗ All historical data weighted equally (old data misleads)
✗ Gets stuck on outdated q* values


================================================================================
QUICK VIVA Q&A
================================================================================

Q: Why does constant step-size work better?
A: Because my environment is non-stationary. The constant α = 0.1 gives 
   exponentially more weight to recent rewards, allowing me to track the 
   drifting q*(a) values. Sample average gets stuck on old data.

Q: What would happen in a stationary environment?
A: Sample average would converge to true values and eventually outperform
   constant α, because all historical data is equally valuable when q*(a)
   doesn't change.

Q: What if I set α = 1?
A: Q(a) would equal only the most recent reward. Too noisy and unstable,
   though very responsive to changes.

Q: What if I set ε = 0?
A: Pure exploitation (greedy). I would never discover better actions that
   become optimal due to drift. Performance would be poor.

Q: Why 500 runs?
A: To average out randomness and get statistically reliable results. Single
   runs are too noisy due to random q* drift and reward variance.

Q: How does non-stationarity work?
A: At each step, all q*(a) values drift by adding N(0, 0.01). This creates
   a random walk where the optimal action changes over time.


================================================================================
CONVERGENCE THEORY
================================================================================

For convergence to true values, step-size needs:
1. Σ αₙ = ∞     (sum to infinity - ensures enough exploration)
2. Σ αₙ² < ∞    (squared sum finite - variance decreases to zero)

Sample Average (αₙ = 1/n):
✓ Both conditions satisfied
→ Converges in stationary environments

Constant (αₙ = 0.1):
✓ First condition satisfied
✗ Second condition violated
→ Doesn't converge, but tracks non-stationary targets


================================================================================
MY CONCLUSIONS
================================================================================

What I Demonstrated:
--------------------
My experiment clearly shows the fundamental RL tradeoff:

STATIONARY → Sample Average (converges to true values)
NON-STATIONARY → Constant Step-Size (tracks changing values)

The graphs prove that constant α = 0.1 significantly outperforms sample
averaging in non-stationary environments, achieving:
- 2-3x higher average reward
- 2x better optimal action selection
- Stable performance vs degrading performance

Why It Matters:
---------------
Real-world problems are often non-stationary:
- User preferences change
- Market conditions evolve  
- System dynamics shift

My experiment demonstrates why adaptive learning (constant α) is crucial
for these scenarios.

Core RL Concepts Learned:
--------------------------
✓ Exploration vs exploitation tradeoff
✓ Action-value estimation and incremental updates
✓ Importance of step-size in non-stationary environments
✓ Exponential recency-weighted averaging
✓ Balancing stability vs adaptability

These concepts extend to complex RL: MDPs, Q-learning, Policy Gradients.

================================================================================
END OF DOCUMENT
================================================================================
================================================================================

Here's the flowchart showing how my experiment works:

┌─────────────────────────────────────────────────────────────────────────┐
│                         START EXPERIMENT                                 │
│                    (Run for 500 independent trials)                      │
└────────────────────────────────┬────────────────────────────────────────┘
                                 │
                    ┌────────────▼────────────┐
                    │  Initialize for Run r:  │
                    │  - q*(a) = [0,0,0,0]   │
                    │  - Q(a) = [0,0,0,0]    │
                    │  - N(a) = [0,0,0,0]    │
                    └────────────┬────────────┘
                                 │
                    ┌────────────▼────────────┐
                    │   For t = 1 to 10,000   │
                    └────────────┬────────────┘
                                 │
                    ┌────────────▼──────────────────────┐
                    │  EPSILON-GREEDY ACTION SELECTION  │
                    │                                   │
                    │  Generate random number r ∈ [0,1] │
                    └────────────┬──────────────────────┘
                                 │
                ┌────────────────┴────────────────┐
                │                                 │
         r < ε (0.1)?                      r >= ε (0.1)?
                │                                 │
        ┌───────▼────────┐              ┌────────▼────────┐
        │    EXPLORE     │              │    EXPLOIT      │
        │ Select random  │              │ a = argmax Q(a) │
        │ action a ∈ {0,3}│              │ (greedy action) │
        └───────┬────────┘              └────────┬────────┘
                │                                 │
                └────────────┬────────────────────┘
                             │
                ┌────────────▼──────────────┐
                │  OBSERVE REWARD           │
                │  R ~ N(q*(a), 1)          │
                │  (normal distribution)    │
                └────────────┬──────────────┘
                             │
                ┌────────────▼──────────────┐
                │  TRACK PERFORMANCE        │
                │  - Store reward R         │
                │  - Check if a = argmax q* │
                └────────────┬──────────────┘
                             │
                ┌────────────▼──────────────┐
                │  UPDATE Q-VALUE           │
                │  N(a) = N(a) + 1          │
                │                           │
                │  If Sample Average:       │
                │    α = 1/N(a)             │
                │  If Constant:             │
                │    α = 0.1                │
                │                           │
                │  Q(a) ← Q(a) + α[R-Q(a)]  │
                └────────────┬──────────────┘
                             │
                ┌────────────▼──────────────┐
                │  NON-STATIONARY DRIFT     │
                │  For each arm i:          │
                │  q*(i) += N(0, 0.01)      │
                │  (random walk)            │
                └────────────┬──────────────┘
                             │
                             │
                    ┌────────▼────────┐
                    │  t = t + 1      │
                    │  t > 10,000?    │
                    └────────┬────────┘
                             │
                    ┌────────┴────────┐
                  No│                 │Yes
                    │                 │
             Loop back          Continue to
             to "For t"         next run
                                     │
                    ┌────────────────▼────────────────┐
                    │  After 500 runs complete:       │
                    │  - Average all rewards          │
                    │  - Average all optimal actions  │
                    └────────────────┬────────────────┘
                                     │
                    ┌────────────────▼────────────────┐
                    │  GENERATE PLOTS                 │
                    │  1. Avg Reward vs Steps         │
                    │  2. % Optimal Action vs Steps   │
                    └────────────────┬────────────────┘
                                     │
                    ┌────────────────▼────────────────┐
                    │            END                  │
                    └─────────────────────────────────┘


KEY DECISION POINTS IN THE FLOWCHART:
--------------------------------------
1. Exploration vs Exploitation: With probability ε, I explore (random action);
   otherwise, I exploit my current knowledge (greedy action).

2. Step-Size Selection: This is where the two methods differ:
   - Sample Average dynamically decreases α as more samples are collected
   - Constant keeps α fixed at 0.1 throughout

3. Non-Stationary Update: After every action, the true values drift, making
   it crucial to adapt quickly (which constant α does better).


================================================================================
1. THEORETICAL BACKGROUND
================================================================================

1.1 MULTI-ARMED BANDIT PROBLEM
-------------------------------
In my experiment, I'm working with the k-armed bandit problem, which is a 
fundamental problem in reinforcement learning. Here's how I understand it:

- At each time step t, I (the agent) select an action At
- The environment returns a reward Rt based on my action
- Each action a has a true value q*(a) = E[Rt | At = a]
- My goal is to learn which actions give the highest expected reward

This is the simplest RL problem - I have a single state with multiple actions.


1.2 STATIONARY vs NON-STATIONARY PROBLEMS
-----------------------------------------
Let me explain the difference:

STATIONARY: True action values q*(a) remain constant over time.
- Past rewards are equally relevant for learning
- More data always improves my estimates

NON-STATIONARY: True action values q*(a) change over time.
- Recent rewards are more relevant than old ones
- Old data can mislead me
- I must "forget" old information and track changes

In my experiment:
- I start with q*(a) = 0 for all arms
- At each timestep, ALL q*(a) drift by adding N(0, 0.01)
- This creates a random walk in action values


1.3 ACTION-VALUE ESTIMATION
----------------------------
I maintain estimates Q(a) ≈ q*(a) for each action's value.

My INCREMENTAL UPDATE RULE is:
Q(a) ← Q(a) + α[R - Q(a)]

Where:
- Q(a) = my current estimate of action a's value
- R = the reward I observed
- α = step-size parameter (my learning rate)
- [R - Q(a)] = prediction error (how much I was wrong)

This is equivalent to: NewEstimate ← OldEstimate + StepSize × Error


1.4 EPSILON-GREEDY EXPLORATION
-------------------------------
I balance exploration vs exploitation using this strategy:
- With probability ε: I select a random action (EXPLORE)
- With probability 1-ε: I select the action with highest Q(a) (EXPLOIT)

In my experiment: ε = 0.1 (I explore 10% of the time, exploit 90%)


================================================================================
2. THE TWO METHODS I'M COMPARING
================================================================================

2.1 SAMPLE AVERAGE METHOD (α = 1/n)
------------------------------------
This is the traditional averaging approach where α decreases over time.

Update Rule I use: Qt+1(a) = Qt(a) + (1/n)[R - Qt(a)]

where n = number of times I've selected action a.

CHARACTERISTICS I observe:
✓ Converges to true mean in stationary problems
✓ All past rewards weighted equally: Q(a) = average of all rewards
✗ Step-size α → 0 as n → ∞ (my learning slows down)
✗ Cannot track non-stationary targets
✗ Old rewards influence my estimates equally as new ones

MATHEMATICAL EXPANSION:
Qn = Qn-1 + (1/n)[Rn - Qn-1]
   = (1/n)Rn + (n-1)/n × Qn-1
   = (1/n)Rn + (n-1)/n × [(1/(n-1))Rn-1 + ... ]
   = (1/n)[R1 + R2 + ... + Rn]  (simple average)

Each reward has weight 1/n (equal weighting).


2.2 CONSTANT STEP-SIZE METHOD (α = 0.1)
----------------------------------------
This is a fixed learning rate that gives more weight to recent rewards.

Update Rule I use: Qt+1(a) = Qt(a) + 0.1[R - Qt(a)]

CHARACTERISTICS I observe:
✓ Can track non-stationary targets
✓ Recent rewards weighted more heavily (exponential recency weighting)
✓ Continuous adaptation to changes
✗ Never fully converges (always adjusts)
✗ Maintains some variance in my estimates

MATHEMATICAL EXPANSION:
Qn+1 = Qn + α[Rn - Qn]
     = αRn + (1-α)Qn
     = αRn + (1-α)[αRn-1 + (1-α)Qn-1]
     = αRn + α(1-α)Rn-1 + (1-α)²Qn-2
     = αRn + α(1-α)Rn-1 + α(1-α)²Rn-2 + ... + (1-α)ⁿQ1
     
     = (1-α)ⁿQ1 + Σ[i=1 to n] α(1-α)ⁿ⁻ⁱRi

This is an EXPONENTIAL RECENCY-WEIGHTED AVERAGE:
- Weight of Ri = α(1-α)ⁿ⁻ⁱ (decays exponentially with age)
- Sum of weights: (1-α)ⁿ + Σα(1-α)ⁿ⁻ⁱ = 1
- Recent rewards have larger weights


================================================================================
3. WHY I EXPECT CONSTANT α TO PERFORM BETTER IN MY NON-STATIONARY PROBLEM
================================================================================

3.1 THE PROBLEM I SEE WITH SAMPLE AVERAGING
--------------------------------------
As n grows large in my experiment:
- α = 1/n becomes very small (e.g., 1/1000 = 0.001)
- New information barely updates my estimate
- My Q(a) becomes "stuck" on old, outdated information
- I cannot adapt quickly when q*(a) changes

Example: If n=1000, a new reward only changes my Q by 0.1%!


3.2 THE ADVANTAGE I GET WITH CONSTANT α
--------------------------------
With fixed α = 0.1, I observe:
- Every new reward updates my Q by 10% of the error
- I continuously learn and adapt
- I effectively "forget" old information (exponential decay)
- I can track the drifting true values q*(a)

Time constant τ = 1/α = 10 steps
- After ~10 steps, influence of old reward decays to ~37%
- After ~30 steps, influence decays to ~5%


3.3 EFFECTIVE MEMORY WINDOW
----------------------------
For my α = 0.1, the effective memory is approximately 1/α = 10 steps.
I primarily "remember" the last ~10 rewards.

In my non-stationary environment:
- q*(a) drifts by N(0, 0.01) each step
- After 10 steps, cumulative drift ≈ √10 × 0.01 ≈ 0.032
- Constant α can track this drift
- Sample average gets overwhelmed by historical data


================================================================================
4. EXPECTED EXPERIMENTAL RESULTS
================================================================================

4.1 AVERAGE REWARD PLOT
------------------------
EXPECTED OBSERVATION:
- Constant α (0.1): Higher average reward, relatively stable
- Sample Average: Lower average reward, potentially declining over time

EXPLANATION:
- Constant α tracks the optimal action better as q*(a) values drift
- Sample average lags behind changes, often selecting suboptimal actions
- The gap widens over time as sample average accumulates more stale data


4.2 OPTIMAL ACTION SELECTION PLOT
----------------------------------
EXPECTED OBSERVATION:
- Constant α (0.1): Higher % of optimal actions (typically 60-80%)
- Sample Average: Lower % of optimal actions (typically 30-50%)

EXPLANATION:
- Optimal action changes as q*(a) values drift
- Constant α adapts quickly to identify new optimal action
- Sample average is slow to update, often selecting previously optimal actions


4.3 TYPICAL PERFORMANCE METRICS
--------------------------------
Method              | Avg Reward | % Optimal Action
--------------------|------------|------------------
Sample Average      | ~0.0-0.3   | 30-50%
Constant α=0.1      | ~0.5-0.8   | 60-80%


================================================================================
5. KEY INSIGHTS FOR MY VIVA
================================================================================

5.1 FUNDAMENTAL UNDERSTANDING
------------------------------
Q: Why does constant step-size work better?
A: In my non-stationary problem, recent information is more valuable than old 
   information. Constant α gives exponentially more weight to recent rewards,
   allowing me to track changes in the environment.

Q: When would sample averaging be better?
A: In stationary problems where q*(a) don't change. There, averaging all past
   rewards gives the best estimate, and the law of large numbers guarantees
   convergence to the true value.

Q: What is the exploration-exploitation tradeoff?
A: Exploration gathers information about action values (I try different arms).
   Exploitation uses my current knowledge to maximize reward (I select the best known arm).
   My ε-greedy strategy balances this: I explore ε% of time, exploit (1-ε)% of time.


5.2 MATHEMATICAL CONCEPTS I UNDERSTAND
--------------------------
Q: What is the incremental update rule?
A: Qn+1 = Qn + α[Rn - Qn]
   It's computationally efficient (I don't need to store all past rewards)
   and mathematically equivalent to averaging for α = 1/n.

Q: What does α control?
A: The learning rate or step-size. It determines how much new information
   affects my current estimate. Higher α = faster learning, more volatility.
   Lower α = slower learning, more stability.

Q: Why is it called "exponential recency-weighted average"?
A: Because the weight of a reward Ri decays exponentially as: α(1-α)^(n-i)
   Older rewards have exponentially smaller influence on my estimates.


5.3 MY EXPERIMENTAL DESIGN CHOICES
------------------------
Q: Why did I choose 10,000 steps?
A: This is enough to observe long-term behavior and see performance differences.
   With σ=0.01 drift, q* values undergo significant random walk over 10k steps.

Q: Why did I choose 4 arms instead of 2 or 10?
A: I wanted a balance between complexity and clarity. 4 arms shows the problem clearly
   without excessive computation or overly simple results.

Q: Why did I set ε = 0.1?
A: This is a standard value providing reasonable exploration (10%) while still mostly
   exploiting (90%). Too low (<0.01) might miss better actions; too high (>0.3)
   would waste time on random exploration.

Q: Why did I run 500 independent runs?
A: To get statistically reliable results by averaging out the randomness in
   rewards and q* drift across multiple trials.


5.4 PRACTICAL IMPLICATIONS I UNDERSTAND
---------------------------
1. Real-world systems are often non-stationary:
   - User preferences change over time
   - Market conditions evolve
   - System dynamics shift
   
2. My experiment shows that adaptive learning (constant α) is crucial for:
   - Online recommendation systems
   - Adaptive control systems
   - Dynamic resource allocation
   
3. I learned that the right α depends on non-stationarity rate:
   - Fast changes → larger α (faster adaptation, more noise)
   - Slow changes → smaller α (more stable, slower adaptation)


5.5 HOW I WOULD ANSWER COMMON VIVA QUESTIONS
------------------------------------
Q: What is reinforcement learning?
A: It's learning by interaction with an environment to maximize cumulative reward.
   The agent (me) takes actions, receives rewards, and learns optimal behavior.

Q: How is this different from supervised learning?
A: There are no labeled examples. I must discover good actions through trial and error.
   Feedback is evaluative (how good was my action) not instructive (what I should do).

Q: What is the reward signal in my experiment?
A: It's sampled from N(q*(a), 1) where q*(a) is the true value of my selected action.
   It's noisy (variance=1) so I need multiple samples to estimate q*(a) accurately.

Q: What happens if I set α = 1?
A: My Q(a) would equal the most recent reward (complete recency, no averaging).
   Very noisy, but extremely responsive to changes.

Q: What happens if I set ε = 0?
A: Pure exploitation (greedy). I would get stuck on initially good actions
   and never discover better alternatives. Performance would be poor.

Q: What happens if I set ε = 1?
A: Pure exploration (random). I would never exploit my knowledge, always acting randomly.
   Performance would be poor (just random chance).


================================================================================
6. THEORETICAL GUARANTEES & CONVERGENCE CONDITIONS I STUDIED
================================================================================

For step-size sequence {αn(a)}, convergence to true values requires:

1. Σ αn(a) = ∞        (infinite sum → ensures all states visited infinitely)
2. Σ αn²(a) < ∞       (squared sum finite → variance decreases to zero)

SAMPLE AVERAGE (αn = 1/n):
✓ Σ 1/n = ∞           (diverges)
✓ Σ 1/n² = π²/6       (converges)
→ Guaranteed convergence in stationary problems

CONSTANT STEP-SIZE (αn = 0.1):
✓ Σ 0.1 = ∞           (diverges)
✗ Σ 0.1² = ∞          (diverges)
→ No guaranteed convergence, but can track non-stationary targets


================================================================================
7. MY SUMMARY AND CONCLUSIONS
================================================================================

My experiment demonstrates a fundamental tradeoff in reinforcement learning:

STATIONARY ENVIRONMENTS → Use decreasing step-size (sample average)
- Guarantees convergence to true values
- Benefits from all historical data

NON-STATIONARY ENVIRONMENTS → Use constant step-size (what I implemented)
- Tracks changing values through exponential recency weighting
- Adapts to evolving optimal policies

What I learned:
The multi-armed bandit is the simplest RL problem but teaches core concepts:
- Exploration vs exploitation
- Action-value estimation
- Incremental learning
- Handling non-stationarity

These concepts I studied extend to full RL problems with states, transitions, and 
long-term planning (Markov Decision Processes, Q-learning, Policy Gradients).

In my experiment, I clearly demonstrated that constant step-size (α=0.1) 
significantly outperforms sample averaging in non-stationary environments because
it gives more weight to recent experiences, allowing it to track the drifting
true action values effectively.

================================================================================
END OF DOCUMENT
================================================================================
